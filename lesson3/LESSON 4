from asyncio import tasks
from cgitb import enable
from email.policy import default
import os
from datetime import datetime
from distutils.cmd import Command
from sched import scheduler
import pandas as pd
from airflow.models import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from airflow.models import Variable
from airflow.providers.postgres.hooks.postgres  import PostgresHook
from airflow.providers.postgres.operators.postgres  import PostgresOperator
import psycopg2
from sqlalchemy import create_engine

args = {
    'owner': 'airflow',  # Информация о владельце DAG
    'start_date': dt.datetime(2020, 12, 23),  # Время начала выполнения пайплайна
    'retries': 1,  # Количество повторений в случае неудач
    'retry_delay': dt.timedelta(minutes=1),  # Пауза между повторами
    'depends_on_past': False,  # Запуск DAG зависит ли от успешности окончания предыдущего запуска по расписанию
}

@dag(default_args=args, schedule_interval='* * * * *')
def titanic_pivot_v2():
    def get_path(file_name):
        return os.path.join(os.path.expanduser('~'), file_name)

    @task
    def download_titanic_dataset(ti):
        
        url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'
        df = pd.read_csv(url)      
        out = df.to_json(orient='records')
    
        ti.xcom_push(key='key', value=out)   
    @task
    def pivot_dataset():
        titanic_data = ti.xcom_pull(key='key')
        titanic_df = pd.read_json(titanic_data)
        df = titanic_df.pivot_table(index=['Sex'],
                                    columns=['Pclass'],
                                    values='Name',
                                    aggfunc='count').reset_index()
        engine = create_engine('postgresql+psycopg2://airflow:airflow@localhost:5432/airflow')
        return df.to_sql(f'{Variable.get("var")}', engine)
    @task
    def mean_fare_per_class():
        titanic_data = ti.xcom_pull(key='key')
        titanic_mean_df = pd.read_json(titanic_data)
        df2 = titanic_mean_df.set_index('Pclass')[['Fare']].stack().mean(level=0)
        engine = create_engine('postgresql+psycopg2://airflow:airflow@localhost:5432/airflow')
        return df2.to_sql(f'{Variable.get("var2")}', engine)

    download_titanic = download_titanic_dataset()
    pivot = pivot_dataset()
    mean =mean_fare_per_class()


dag = titanic_pivot_v2()
